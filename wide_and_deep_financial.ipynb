{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_jzX--TgKgj"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WLUz4MogbLI"
   },
   "source": [
    "This Jupyter Notebook report shows an application of the wide and deep learning model proposed by Google. The wide and deep model consists of two parts: a linear classifier with ReLU as the activation function (wide), and a feed-forward neural network (deep). The purpose of combining these two models is to make use of the advantages of both models: wide for memorization and deep for generalization. In this report, I will be doing a financial data trend prediction using partial data from the [NASDAQ 100 stock dataset](http://cseweb.ucsd.edu/~yaq007/nasdaq100.zip). The following of this report will walk through all the process from data preparation, neural network construction and result evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAQYIBmaoCCs"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzJrGXZNloIX"
   },
   "source": [
    "First of all, import all necessary python packages such as tensorflow and numpy. The matplotlib was also imported in order to visualize the trend of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFet8-uXn8zM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2zoyQi9pLFZ"
   },
   "source": [
    "The dataset file was provided as a local text file with the name \"aal.txt\". Import the dataset file and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hG_zJBUHoi5Q"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "FILE_NAME = 'aal.txt'\n",
    "#if not tf.gfile.Exists(FILE_NAME):\n",
    "#  uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vsi4fc_no49C",
    "outputId": "fbaa0841-7801-4a62-ce2b-d444877a1a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.98  , 35.931 , 35.9044, ..., 48.53  , 48.56  , 48.56  ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(FILE_NAME, 'r') as fid:\n",
    "  lines = fid.readlines()\n",
    "  \n",
    "lines = [line.split('\\r')[0] for line in lines]\n",
    "lines = [line.split('\\n')[0] for line in lines]\n",
    "while '' in lines:\n",
    "  lines.remove('')\n",
    "data = np.asarray(list(map(float, lines)))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QTPg1ugpH5j"
   },
   "source": [
    "Plot the data to visualize the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "kpFL8fOTpJNp",
    "outputId": "c85aad1a-99c2-4f5c-9cdb-0464419a3342"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3pFwEiEoRZD"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLYZSkYJpbMi"
   },
   "source": [
    "Before building up the neural network and feeding the data into the network, I split the data into training and testing sets. Three quarters of all the data were used for training and the rest quarter were used for testing and evaluating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52QSs1BDpWYr"
   },
   "outputs": [],
   "source": [
    "data_length = len(data)\n",
    "train_length = int(0.75 * data_length)\n",
    "test_length = data_length - train_length\n",
    "train_data = data[:train_length]\n",
    "test_data = data[train_length:data_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d7b163trFtf"
   },
   "source": [
    "Since the goal of this project is to predict the trend from looking at a certain time interval of data, I classified the data into two classes: increasing (represented as class 1) and decreasing (represented as class 0). The flattened train data is reshaped into many groups of data segments with length 60, representing the data within 60 minutes. The predicted trend of the financial data is the trend in the past 60 minutes. Finally, I converted the class labels to one-hot encoded vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irZa8j47p3nM"
   },
   "outputs": [],
   "source": [
    "train_data_norm = (train_data - np.mean(train_data)) / np.std(train_data)\n",
    "test_data_norm = (test_data - np.mean(test_data)) / np.std(test_data)\n",
    "\n",
    "# Reshape and prepare for the training data and labels\n",
    "window_size = 60\n",
    "X_train = train_data_norm.reshape(-1, window_size)\n",
    "train_labels = np.sign(X_train[:,-1] - X_train[:,0])\n",
    "\n",
    "# Convert the two-class labels to one-hot encoded vectors\n",
    "train_labels[np.where(train_labels==-1)] = 0\n",
    "y_train_onehot = np.eye(2)[train_labels.astype(int)]\n",
    "\n",
    "# Reshape and prepare for the testing data and labels\n",
    "X_test = test_data_norm.reshape(-1, window_size)\n",
    "test_labels = np.sign(X_test[:,-1] - X_test[:,0])\n",
    "\n",
    "# Convert the two-class labels to one-hot encoded vectors\n",
    "test_labels[np.where(test_labels==-1)] = 0\n",
    "y_test_onehot = np.eye(2)[test_labels.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o08bowlhoUKj"
   },
   "source": [
    "# Construct the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLLk3O-g6eEF"
   },
   "source": [
    "The wide and deep neural network was built using keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7GHfKfmiubYw",
    "outputId": "631bbac3-622c-471b-adf2-15de3d179235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSTG4OJaSkfu"
   },
   "source": [
    "The wide neural network was a simple linear classifier with ReLU activation function. The input layer has 60 nodes representing time series data within 60 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "MIWDBv7lujax",
    "outputId": "0f9bab00-a641-46e7-c4c8-81db0c20a943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "common_input (InputLayer)    (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "wide_out (Dense)             (None, 2)                 122       \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Wide\n",
    "common_input = Input(shape=X_train[1].shape, name='common_input')\n",
    "wide_out = Dense(2, activation='relu', name='wide_out')(common_input)\n",
    "wide_model = Model(inputs=common_input, outputs=wide_out)\n",
    "wide_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xw_oWYsLTLYu"
   },
   "source": [
    "The deep neural network was a feed-forward neural network with 2 hidden layers with 125 nodes and 50 nodes, and 1 output layer with 2 nodes representing 2 classes. Each layer has a ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "bJLksZWrwS2Q",
    "outputId": "d22cc29c-e2d0-49c6-a3f0-313921d47908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "common_input (InputLayer)    (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "deep1 (Dense)                (None, 125)               7625      \n",
      "_________________________________________________________________\n",
      "deep2 (Dense)                (None, 50)                6300      \n",
      "_________________________________________________________________\n",
      "deep_out (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 14,027\n",
      "Trainable params: 14,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Deep\n",
    "deep1 = Dense(125, activation='relu', name='deep1')(common_input)\n",
    "deep2 = Dense(50, activation='relu', name='deep2')(deep1)\n",
    "deep_out = Dense(2, activation='relu', name='deep_out')(deep2)\n",
    "deep_model = Model(inputs=common_input, outputs=deep_out)\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AT56ba5-ARKR"
   },
   "source": [
    "Concatenate the outputs of both wide and deep network and put them into a single layer perceptron with a softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "tmAjWalEwZdZ",
    "outputId": "165f251b-2e20-41bd-d0f5-cd828af7019a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "common_input (InputLayer)       (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep1 (Dense)                   (None, 125)          7625        common_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deep2 (Dense)                   (None, 50)           6300        deep1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "wide_out (Dense)                (None, 2)            122         common_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "deep_out (Dense)                (None, 2)            102         deep2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "wide_and_deep_in (Concatenate)  (None, 4)            0           wide_out[0][0]                   \n",
      "                                                                 deep_out[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "wide_and_deep_out (Dense)       (None, 2)            10          wide_and_deep_in[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 14,159\n",
      "Trainable params: 14,159\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Concatenate wide and deep\n",
    "wide_and_deep_in = concatenate([wide_out, deep_out], name='wide_and_deep_in')\n",
    "wide_and_deep_out = Dense(2, activation='softmax', name='wide_and_deep_out')(wide_and_deep_in)\n",
    "wide_and_deep_model = Model(inputs=common_input, outputs=wide_and_deep_out)\n",
    "wide_and_deep_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WD9VKwm3uOxx"
   },
   "source": [
    "# Train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCd36J79Aohs"
   },
   "source": [
    "Since this is a two-class classification problem, I utilized the binary cross-entropy loss function and adaptive gradient update optimizer. The metrics for evaluation was the accuracy. After that, I trained the neural network with a batch size of 32 and I ran this process for 200 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gI9uQlGlwm77"
   },
   "outputs": [],
   "source": [
    "wide_and_deep_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6754
    },
    "colab_type": "code",
    "id": "ChAdoYw9wqmu",
    "outputId": "5c7d2369-07a4-4d15-9d7a-4e05aa012991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "507/507 [==============================] - 0s 661us/step - loss: 0.7277 - acc: 0.5089\n",
      "Epoch 2/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.6883 - acc: 0.5424\n",
      "Epoch 3/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.7127 - acc: 0.5542\n",
      "Epoch 4/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.6797 - acc: 0.5957\n",
      "Epoch 5/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6752 - acc: 0.5799\n",
      "Epoch 6/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.6814 - acc: 0.5602\n",
      "Epoch 7/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.6801 - acc: 0.5819\n",
      "Epoch 8/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.6771 - acc: 0.5444\n",
      "Epoch 9/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.6598 - acc: 0.6055\n",
      "Epoch 10/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6575 - acc: 0.6036\n",
      "Epoch 11/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.6691 - acc: 0.5740\n",
      "Epoch 12/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.6538 - acc: 0.5937\n",
      "Epoch 13/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6560 - acc: 0.6154\n",
      "Epoch 14/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.6465 - acc: 0.6095\n",
      "Epoch 15/200\n",
      "507/507 [==============================] - 0s 77us/step - loss: 0.6370 - acc: 0.6489\n",
      "Epoch 16/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.6499 - acc: 0.6351\n",
      "Epoch 17/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.6540 - acc: 0.6430\n",
      "Epoch 18/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6360 - acc: 0.6430\n",
      "Epoch 19/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6229 - acc: 0.6746\n",
      "Epoch 20/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.6437 - acc: 0.6213\n",
      "Epoch 21/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.6320 - acc: 0.6430\n",
      "Epoch 22/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.6282 - acc: 0.6627\n",
      "Epoch 23/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.6222 - acc: 0.6943\n",
      "Epoch 24/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.6318 - acc: 0.6647\n",
      "Epoch 25/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.6123 - acc: 0.7002\n",
      "Epoch 26/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.6097 - acc: 0.6963\n",
      "Epoch 27/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.6166 - acc: 0.6884\n",
      "Epoch 28/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.6003 - acc: 0.7002\n",
      "Epoch 29/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.6081 - acc: 0.7061\n",
      "Epoch 30/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.6032 - acc: 0.6785\n",
      "Epoch 31/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5941 - acc: 0.7337\n",
      "Epoch 32/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.6020 - acc: 0.7298\n",
      "Epoch 33/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.6036 - acc: 0.7337\n",
      "Epoch 34/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5871 - acc: 0.7199\n",
      "Epoch 35/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.6063 - acc: 0.7061\n",
      "Epoch 36/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.6017 - acc: 0.6982\n",
      "Epoch 37/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5962 - acc: 0.7061\n",
      "Epoch 38/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5967 - acc: 0.7041\n",
      "Epoch 39/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5815 - acc: 0.7278\n",
      "Epoch 40/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.5799 - acc: 0.7554\n",
      "Epoch 41/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5887 - acc: 0.7258\n",
      "Epoch 42/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.5780 - acc: 0.7377\n",
      "Epoch 43/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5819 - acc: 0.7298\n",
      "Epoch 44/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5842 - acc: 0.7140\n",
      "Epoch 45/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5730 - acc: 0.7416\n",
      "Epoch 46/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5783 - acc: 0.7357\n",
      "Epoch 47/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5915 - acc: 0.7416\n",
      "Epoch 48/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5775 - acc: 0.7278\n",
      "Epoch 49/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5742 - acc: 0.7653\n",
      "Epoch 50/200\n",
      "507/507 [==============================] - 0s 49us/step - loss: 0.5670 - acc: 0.7515\n",
      "Epoch 51/200\n",
      "507/507 [==============================] - 0s 79us/step - loss: 0.5627 - acc: 0.7416\n",
      "Epoch 52/200\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.5619 - acc: 0.843 - 0s 57us/step - loss: 0.5662 - acc: 0.7613\n",
      "Epoch 53/200\n",
      "507/507 [==============================] - 0s 49us/step - loss: 0.5667 - acc: 0.7633\n",
      "Epoch 54/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5762 - acc: 0.7377\n",
      "Epoch 55/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5651 - acc: 0.7554\n",
      "Epoch 56/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5577 - acc: 0.7673\n",
      "Epoch 57/200\n",
      "507/507 [==============================] - 0s 45us/step - loss: 0.5651 - acc: 0.7791\n",
      "Epoch 58/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5647 - acc: 0.7377\n",
      "Epoch 59/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5627 - acc: 0.7751\n",
      "Epoch 60/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5655 - acc: 0.7712\n",
      "Epoch 61/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5643 - acc: 0.7416\n",
      "Epoch 62/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5580 - acc: 0.7633\n",
      "Epoch 63/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5582 - acc: 0.7594\n",
      "Epoch 64/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5595 - acc: 0.7850\n",
      "Epoch 65/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.5488 - acc: 0.7909\n",
      "Epoch 66/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5622 - acc: 0.7692\n",
      "Epoch 67/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5485 - acc: 0.7890\n",
      "Epoch 68/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5510 - acc: 0.7968\n",
      "Epoch 69/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5645 - acc: 0.7535\n",
      "Epoch 70/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5699 - acc: 0.7791\n",
      "Epoch 71/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.5484 - acc: 0.8028\n",
      "Epoch 72/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5468 - acc: 0.7791\n",
      "Epoch 73/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.5518 - acc: 0.7968\n",
      "Epoch 74/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.5503 - acc: 0.7909\n",
      "Epoch 75/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5537 - acc: 0.7890\n",
      "Epoch 76/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5435 - acc: 0.8008\n",
      "Epoch 77/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5396 - acc: 0.7870\n",
      "Epoch 78/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5454 - acc: 0.8107\n",
      "Epoch 79/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5407 - acc: 0.8087\n",
      "Epoch 80/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5401 - acc: 0.7949\n",
      "Epoch 81/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.5440 - acc: 0.8047\n",
      "Epoch 82/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.5429 - acc: 0.8028\n",
      "Epoch 83/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5456 - acc: 0.7909\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 0s 61us/step - loss: 0.5500 - acc: 0.7811\n",
      "Epoch 85/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5328 - acc: 0.8205\n",
      "Epoch 86/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5501 - acc: 0.8008\n",
      "Epoch 87/200\n",
      "507/507 [==============================] - 0s 99us/step - loss: 0.5364 - acc: 0.8067\n",
      "Epoch 88/200\n",
      "507/507 [==============================] - 0s 91us/step - loss: 0.5371 - acc: 0.8047\n",
      "Epoch 89/200\n",
      "507/507 [==============================] - 0s 87us/step - loss: 0.5417 - acc: 0.7949\n",
      "Epoch 90/200\n",
      "507/507 [==============================] - 0s 97us/step - loss: 0.5382 - acc: 0.8166\n",
      "Epoch 91/200\n",
      "507/507 [==============================] - 0s 99us/step - loss: 0.5350 - acc: 0.8087\n",
      "Epoch 92/200\n",
      "507/507 [==============================] - 0s 101us/step - loss: 0.5438 - acc: 0.7870\n",
      "Epoch 93/200\n",
      "507/507 [==============================] - 0s 104us/step - loss: 0.5347 - acc: 0.7949\n",
      "Epoch 94/200\n",
      "507/507 [==============================] - 0s 99us/step - loss: 0.5326 - acc: 0.8047\n",
      "Epoch 95/200\n",
      "507/507 [==============================] - 0s 103us/step - loss: 0.5301 - acc: 0.8166\n",
      "Epoch 96/200\n",
      "507/507 [==============================] - 0s 103us/step - loss: 0.5317 - acc: 0.8107\n",
      "Epoch 97/200\n",
      "507/507 [==============================] - 0s 81us/step - loss: 0.5245 - acc: 0.8225\n",
      "Epoch 98/200\n",
      "507/507 [==============================] - 0s 81us/step - loss: 0.5413 - acc: 0.8087\n",
      "Epoch 99/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5281 - acc: 0.8245\n",
      "Epoch 100/200\n",
      "507/507 [==============================] - 0s 75us/step - loss: 0.5431 - acc: 0.8107\n",
      "Epoch 101/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5295 - acc: 0.8087\n",
      "Epoch 102/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5335 - acc: 0.8146\n",
      "Epoch 103/200\n",
      "507/507 [==============================] - 0s 47us/step - loss: 0.5250 - acc: 0.8245\n",
      "Epoch 104/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5365 - acc: 0.8008\n",
      "Epoch 105/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5251 - acc: 0.8304\n",
      "Epoch 106/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5358 - acc: 0.8126\n",
      "Epoch 107/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5289 - acc: 0.8205\n",
      "Epoch 108/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5290 - acc: 0.8185\n",
      "Epoch 109/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5290 - acc: 0.8146\n",
      "Epoch 110/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5282 - acc: 0.8067\n",
      "Epoch 111/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5304 - acc: 0.8166\n",
      "Epoch 112/200\n",
      "507/507 [==============================] - 0s 75us/step - loss: 0.5207 - acc: 0.8225\n",
      "Epoch 113/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5199 - acc: 0.8225\n",
      "Epoch 114/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.5227 - acc: 0.8245\n",
      "Epoch 115/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5206 - acc: 0.8245\n",
      "Epoch 116/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5193 - acc: 0.8146\n",
      "Epoch 117/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5176 - acc: 0.8343\n",
      "Epoch 118/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5194 - acc: 0.8225\n",
      "Epoch 119/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5213 - acc: 0.8225\n",
      "Epoch 120/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5165 - acc: 0.8225\n",
      "Epoch 121/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5230 - acc: 0.8107\n",
      "Epoch 122/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5203 - acc: 0.8264\n",
      "Epoch 123/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5310 - acc: 0.8126\n",
      "Epoch 124/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.5272 - acc: 0.8087\n",
      "Epoch 125/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5240 - acc: 0.8185\n",
      "Epoch 126/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5160 - acc: 0.8284\n",
      "Epoch 127/200\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.5044 - acc: 0.812 - 0s 61us/step - loss: 0.5181 - acc: 0.8284\n",
      "Epoch 128/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5120 - acc: 0.8284\n",
      "Epoch 129/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5131 - acc: 0.8264\n",
      "Epoch 130/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5201 - acc: 0.8284\n",
      "Epoch 131/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5165 - acc: 0.8304\n",
      "Epoch 132/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5089 - acc: 0.8383\n",
      "Epoch 133/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.5120 - acc: 0.8284\n",
      "Epoch 134/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5125 - acc: 0.8402\n",
      "Epoch 135/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5097 - acc: 0.8284\n",
      "Epoch 136/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5108 - acc: 0.8225\n",
      "Epoch 137/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.5110 - acc: 0.8304\n",
      "Epoch 138/200\n",
      "507/507 [==============================] - 0s 81us/step - loss: 0.5124 - acc: 0.8284\n",
      "Epoch 139/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5189 - acc: 0.8245\n",
      "Epoch 140/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.5043 - acc: 0.8343\n",
      "Epoch 141/200\n",
      "507/507 [==============================] - 0s 81us/step - loss: 0.5138 - acc: 0.8264\n",
      "Epoch 142/200\n",
      "507/507 [==============================] - 0s 73us/step - loss: 0.5142 - acc: 0.8284\n",
      "Epoch 143/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5109 - acc: 0.8225\n",
      "Epoch 144/200\n",
      "507/507 [==============================] - 0s 83us/step - loss: 0.5211 - acc: 0.8304\n",
      "Epoch 145/200\n",
      "507/507 [==============================] - 0s 75us/step - loss: 0.5105 - acc: 0.8304\n",
      "Epoch 146/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.5081 - acc: 0.8323\n",
      "Epoch 147/200\n",
      "507/507 [==============================] - 0s 69us/step - loss: 0.5161 - acc: 0.8363\n",
      "Epoch 148/200\n",
      "507/507 [==============================] - 0s 81us/step - loss: 0.5032 - acc: 0.8323\n",
      "Epoch 149/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5128 - acc: 0.8304\n",
      "Epoch 150/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5030 - acc: 0.8343\n",
      "Epoch 151/200\n",
      "507/507 [==============================] - 0s 65us/step - loss: 0.5026 - acc: 0.8402\n",
      "Epoch 152/200\n",
      "507/507 [==============================] - 0s 77us/step - loss: 0.5057 - acc: 0.8225\n",
      "Epoch 153/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4976 - acc: 0.8481\n",
      "Epoch 154/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5013 - acc: 0.8323\n",
      "Epoch 155/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.5092 - acc: 0.8205\n",
      "Epoch 156/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4978 - acc: 0.8304\n",
      "Epoch 157/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5081 - acc: 0.8185\n",
      "Epoch 158/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.5042 - acc: 0.8402\n",
      "Epoch 159/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.5011 - acc: 0.8402\n",
      "Epoch 160/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4945 - acc: 0.8363\n",
      "Epoch 161/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5128 - acc: 0.8323\n",
      "Epoch 162/200\n",
      "507/507 [==============================] - 0s 49us/step - loss: 0.4996 - acc: 0.8383\n",
      "Epoch 163/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5008 - acc: 0.8462\n",
      "Epoch 164/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4980 - acc: 0.8402\n",
      "Epoch 165/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.4915 - acc: 0.8442\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 0s 55us/step - loss: 0.4991 - acc: 0.8363\n",
      "Epoch 167/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.4985 - acc: 0.8442\n",
      "Epoch 168/200\n",
      "507/507 [==============================] - 0s 47us/step - loss: 0.5000 - acc: 0.8383\n",
      "Epoch 169/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.5019 - acc: 0.8264\n",
      "Epoch 170/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.4997 - acc: 0.8363\n",
      "Epoch 171/200\n",
      "507/507 [==============================] - 0s 57us/step - loss: 0.5019 - acc: 0.8323\n",
      "Epoch 172/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4960 - acc: 0.8284\n",
      "Epoch 173/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5000 - acc: 0.8462\n",
      "Epoch 174/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4934 - acc: 0.8383\n",
      "Epoch 175/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.4950 - acc: 0.8481\n",
      "Epoch 176/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4940 - acc: 0.8323\n",
      "Epoch 177/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.4867 - acc: 0.8462\n",
      "Epoch 178/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.4979 - acc: 0.8323\n",
      "Epoch 179/200\n",
      "507/507 [==============================] - 0s 51us/step - loss: 0.4908 - acc: 0.8422\n",
      "Epoch 180/200\n",
      "507/507 [==============================] - 0s 71us/step - loss: 0.4883 - acc: 0.8462\n",
      "Epoch 181/200\n",
      "507/507 [==============================] - 0s 47us/step - loss: 0.4903 - acc: 0.8422\n",
      "Epoch 182/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4933 - acc: 0.8462\n",
      "Epoch 183/200\n",
      "507/507 [==============================] - 0s 59us/step - loss: 0.5040 - acc: 0.8323\n",
      "Epoch 184/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.4904 - acc: 0.8343\n",
      "Epoch 185/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4967 - acc: 0.8383\n",
      "Epoch 186/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4992 - acc: 0.8304\n",
      "Epoch 187/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4941 - acc: 0.8383\n",
      "Epoch 188/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4992 - acc: 0.8343\n",
      "Epoch 189/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.4897 - acc: 0.8383\n",
      "Epoch 190/200\n",
      "507/507 [==============================] - 0s 53us/step - loss: 0.4825 - acc: 0.8481\n",
      "Epoch 191/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4905 - acc: 0.8383\n",
      "Epoch 192/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4810 - acc: 0.8402\n",
      "Epoch 193/200\n",
      "507/507 [==============================] - 0s 67us/step - loss: 0.4915 - acc: 0.8383\n",
      "Epoch 194/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4869 - acc: 0.8343\n",
      "Epoch 195/200\n",
      "507/507 [==============================] - 0s 61us/step - loss: 0.4787 - acc: 0.8363\n",
      "Epoch 196/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4834 - acc: 0.8422\n",
      "Epoch 197/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4896 - acc: 0.8284\n",
      "Epoch 198/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4946 - acc: 0.8264\n",
      "Epoch 199/200\n",
      "507/507 [==============================] - 0s 55us/step - loss: 0.4816 - acc: 0.8402\n",
      "Epoch 200/200\n",
      "507/507 [==============================] - 0s 63us/step - loss: 0.4880 - acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27017e0ccf8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_and_deep_model.fit(X_train, y_train_onehot, batch_size=32, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6u-dhe6EBjvW"
   },
   "source": [
    "The result showed that the training accuracy was about 89%. The training log also showed that each epoch took approximately 0.25 ms on average and running 200 epochs took 50 ms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfQXLFw5NHVk"
   },
   "source": [
    "# Evaluate the testing data accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkOUQWEkJVD1"
   },
   "source": [
    "After training the wide and deep neural network, the testing data and label were fed into the neural network to evaluate the testing data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "uRY4-tgIwv8i",
    "outputId": "5ad0f0e8-c353-46ae-b506-7ceb8baba691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 367us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42130247384486114, 0.863905325443787]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_and_deep_model.evaluate(X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mz0AE52nPeMi"
   },
   "source": [
    "The testing data accuracy was extremely accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bp9W4PAWQhAR"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YQumU6OQrmV"
   },
   "source": [
    "The above testing result has shown that using a wide and deep neural network to predict the trend of a time series financial data is pretty accurate. The wide network memorizes the up-and-down patterns and the deep network generalizes the conditions so that the whole model can still generate accurate data when being fed some totally new data to the model. \n",
    "\n",
    "In the future, I might improve the accuracy by increasing the width of the wide side of the model so that more characteristics of the data can be memorized. Meanwhile, the deep side can be added more layers so that more general cases can be covered. Moreover, obtaining the training input data by moving the selection window across all the training data can improve the training and testing accuracy because the current method of getting data block by block might omit some crossed trend patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
