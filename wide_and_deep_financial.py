# -*- coding: utf-8 -*-
"""wide_and_deep_financial.ipynb

Automatically generated by Colaboratory.

"""

"""
Data Preparation

import all necessary python packages such as tensorflow and numpy. The matplotlib was also imported in order to visualize the trend of the data.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

"""
The dataset file was provided as a local text file with the name "aal.txt". Import the dataset file and extract the data
"""

#from google.colab import files
FILE_NAME = 'aal.txt'
#if not tf.gfile.Exists(FILE_NAME):
#  uploaded = files.upload()

with open(FILE_NAME, 'r') as fid:
  lines = fid.readlines()
  
lines = [line.split('\r')[0] for line in lines]
lines = [line.split('\n')[0] for line in lines]
while '' in lines:
  lines.remove('')
data = np.asarray(list(map(float, lines)))
data

"""
Plot the data to visualize the trend
"""

plt.plot(data)
plt.show()

"""
Data Preprocessing

Three quarters of all the data were used for training and the rest quarter were used for testing and evaluating.
"""

data_length = len(data)
train_length = int(0.75 * data_length)
test_length = data_length - train_length
train_data = data[:train_length]
test_data = data[train_length:data_length]

"""
Classify the data into two classes: increasing (represented as class 1) and decreasing (represented as class 0). 

The flattened train data is reshaped into many groups of data segments with length 60, representing the data within 60 minutes. 

The predicted trend of the financial data is the trend in the past 60 minutes. 
 
Convert the class labels to one-hot encoded vectors.
"""

train_data_norm = (train_data - np.mean(train_data)) / np.std(train_data)
test_data_norm = (test_data - np.mean(test_data)) / np.std(test_data)

# Reshape and prepare for the training data and labels
window_size = 60
X_train = train_data_norm.reshape(-1, window_size)
train_labels = np.sign(X_train[:,-1] - X_train[:,0])

# Convert the two-class labels to one-hot encoded vectors
train_labels[np.where(train_labels==-1)] = 0
y_train_onehot = np.eye(2)[train_labels.astype(int)]

# Reshape and prepare for the testing data and labels
X_test = test_data_norm.reshape(-1, window_size)
test_labels = np.sign(X_test[:,-1] - X_test[:,0])

# Convert the two-class labels to one-hot encoded vectors
test_labels[np.where(test_labels==-1)] = 0
y_test_onehot = np.eye(2)[test_labels.astype(int)]

"""
Construct the Neural Network

The wide and deep neural network was built using keras.
"""

from keras.models import Model, Input
from keras.layers.merge import add, concatenate
from keras.layers import Dense

"""
The wide neural network was a simple linear classifier with ReLU activation function. 
The input layer has 60 nodes representing time series data within 60 minutes.
"""

# Wide
common_input = Input(shape=X_train[1].shape, name='common_input')
wide_out = Dense(2, activation='relu', name='wide_out')(common_input)
wide_model = Model(inputs=common_input, outputs=wide_out)
wide_model.summary()

"""
The deep neural network was a feed-forward neural network with 2 hidden layers with 125 nodes and 50 nodes, 
and 1 output layer with 2 nodes representing 2 classes. Each layer has a ReLU activation function
"""

# Deep
deep1 = Dense(125, activation='relu', name='deep1')(common_input)
deep2 = Dense(50, activation='relu', name='deep2')(deep1)
deep_out = Dense(2, activation='relu', name='deep_out')(deep2)
deep_model = Model(inputs=common_input, outputs=deep_out)
deep_model.summary()

"""
Concatenate the outputs of both wide and deep network and put them into a single layer perceptron with a softmax activation function.
"""

# Concatenate wide and deep
wide_and_deep_in = concatenate([wide_out, deep_out], name='wide_and_deep_in')
wide_and_deep_out = Dense(2, activation='softmax', name='wide_and_deep_out')(wide_and_deep_in)
wide_and_deep_model = Model(inputs=common_input, outputs=wide_and_deep_out)
wide_and_deep_model.summary()

"""
Train the Neural Network

Utilize the binary cross-entropy loss function and adaptive gradient update optimizer. 
The metrics for evaluation was the accuracy. Train the neural network with a batch size of 32 for 200 epochs.
"""

wide_and_deep_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])

wide_and_deep_model.fit(X_train, y_train_onehot, batch_size=32, epochs=200)

"""
Evaluate the testing data accuracy

After training the wide and deep neural network, the testing data and label were fed into the neural network to evaluate the testing data accuracy
"""

print(wide_and_deep_model.evaluate(X_test, y_test_onehot))

