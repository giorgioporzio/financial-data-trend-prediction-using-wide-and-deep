# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zynfTdgl_YUli8C7GZygz36ADI7AyitT

# Overview

This Jupyter Notebook report shows an application of the wide and deep learning model proposed by Google. The wide and deep model consists of two parts: a linear classifier with ReLU as the activation function (wide), and a feed-forward neural network (deep). The purpose of combining these two models is to make use of the advantages of both models: wide for memorization and deep for generalization. In this report, I will be doing a financial data trend prediction using partial data from the [NASDAQ 100 stock dataset](http://cseweb.ucsd.edu/~yaq007/nasdaq100.zip). The following of this report will walk through all the process from data preparation, neural network construction and result evaluation.

# Data Preparation

First of all, import all necessary python packages such as tensorflow and numpy. The matplotlib was also imported in order to visualize the trend of the data.
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

"""The dataset file was provided as a local text file with the name "aal.txt". Import the dataset file and extract the data"""

from google.colab import files
FILE_NAME = 'aal.txt'
if not tf.gfile.Exists(FILE_NAME):
  uploaded = files.upload()

with open(FILE_NAME, 'r') as fid:
  lines = fid.readlines()
  
lines = [line.split('\r')[0] for line in lines]
lines = [line.split('\n')[0] for line in lines]
while '' in lines:
  lines.remove('')
data = np.asarray(list(map(float, lines)))
data

"""Plot the data to visualize the trend"""

plt.plot(data)
plt.show()

"""# Data Preprocessing

Before building up the neural network and feeding the data into the network, I split the data into training and testing sets. Three quarters of all the data were used for training and the rest quarter were used for testing and evaluating.
"""

data_length = len(data)
train_length = int(0.75 * data_length)
test_length = data_length - train_length
train_data = data[:train_length]
test_data = data[train_length:data_length]

"""Since the goal of this project is to predict the trend from looking at a certain time interval of data, I classified the data into two classes: increasing (represented as class 1) and decreasing (represented as class 0). The flattened train data is reshaped into many groups of data segments with length 60, representing the data within 60 minutes. The predicted trend of the financial data is the trend in the past 60 minutes. Finally, I converted the class labels to one-hot encoded vectors."""

train_data_norm = (train_data - np.mean(train_data)) / np.std(train_data)
test_data_norm = (test_data - np.mean(test_data)) / np.std(test_data)

# Reshape and prepare for the training data and labels
window_size = 60
X_train = train_data_norm.reshape(-1, window_size)
train_labels = np.sign(X_train[:,-1] - X_train[:,0])

# Convert the two-class labels to one-hot encoded vectors
train_labels[np.where(train_labels==-1)] = 0
y_train_onehot = np.eye(2)[train_labels.astype(int)]

# Reshape and prepare for the testing data and labels
X_test = test_data_norm.reshape(-1, window_size)
test_labels = np.sign(X_test[:,-1] - X_test[:,0])

# Convert the two-class labels to one-hot encoded vectors
test_labels[np.where(test_labels==-1)] = 0
y_test_onehot = np.eye(2)[test_labels.astype(int)]

"""# Construct the Neural Network

The wide and deep neural network was built using keras.
"""

from keras.models import Model, Input
from keras.layers.merge import add, concatenate
from keras.layers import Dense

"""The wide neural network was a simple linear classifier with ReLU activation function. The input layer has 60 nodes representing time series data within 60 minutes."""

# Wide
common_input = Input(shape=X_train[1].shape, name='common_input')
wide_out = Dense(2, activation='relu', name='wide_out')(common_input)
wide_model = Model(inputs=common_input, outputs=wide_out)
wide_model.summary()

"""The deep neural network was a feed-forward neural network with 2 hidden layers with 125 nodes and 50 nodes, and 1 output layer with 2 nodes representing 2 classes. Each layer has a ReLU activation function"""

# Deep
deep1 = Dense(125, activation='relu', name='deep1')(common_input)
deep2 = Dense(50, activation='relu', name='deep2')(deep1)
deep_out = Dense(2, activation='relu', name='deep_out')(deep2)
deep_model = Model(inputs=common_input, outputs=deep_out)
deep_model.summary()

"""Concatenate the outputs of both wide and deep network and put them into a single layer perceptron with a softmax activation function."""

# Concatenate wide and deep
wide_and_deep_in = concatenate([wide_out, deep_out], name='wide_and_deep_in')
wide_and_deep_out = Dense(2, activation='softmax', name='wide_and_deep_out')(wide_and_deep_in)
wide_and_deep_model = Model(inputs=common_input, outputs=wide_and_deep_out)
wide_and_deep_model.summary()

"""# Train the Neural Network

Since this is a two-class classification problem, I utilized the binary cross-entropy loss function and adaptive gradient update optimizer. The metrics for evaluation was the accuracy. After that, I trained the neural network with a batch size of 32 and I ran this process for 200 epochs.
"""

wide_and_deep_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])

wide_and_deep_model.fit(X_train, y_train_onehot, batch_size=32, epochs=200)

"""The result showed that the training accuracy was about 89%. The training log also showed that each epoch took approximately 0.25 ms on average and running 200 epochs took 50 ms.

# Evaluate the testing data accuracy

After training the wide and deep neural network, the testing data and label were fed into the neural network to evaluate the testing data accuracy
"""

wide_and_deep_model.evaluate(X_test, y_test_onehot)

"""The testing data accuracy was extremely accurate.

# Conclusion

The above testing result has shown that using a wide and deep neural network to predict the trend of a time series financial data is pretty accurate. The wide network memorizes the up-and-down patterns and the deep network generalizes the conditions so that the whole model can still generate accurate data when being fed some totally new data to the model. 

In the future, I might improve the accuracy by increasing the width of the wide side of the model so that more characteristics of the data can be memorized. Meanwhile, the deep side can be added more layers so that more general cases can be covered. Moreover, obtaining the training input data by moving the selection window across all the training data can improve the training and testing accuracy because the current method of getting data block by block might omit some crossed trend patterns.
"""